{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rn4v7B1dL8u2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"personnelportfoliodataset.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "df[\"prompt\"] = df[\"prompt\"].apply(clean_text)\n",
        "df[\"response\"] = df[\"response\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "46EnUNngMhek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True,)\n",
        "test_df = test_df.reset_index(drop=True,)"
      ],
      "metadata": {
        "id": "18my8jx-OkBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "fkupHFyNM-bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(example):\n",
        "  input = tokenizer(example[\"prompt\"], truncation=True, padding=\"max_length\", max_length=500)\n",
        "  target = tokenizer(example[\"response\"], truncation=True, padding=\"max_length\", max_length=500)\n",
        "\n",
        "  input[\"labels\"] = target[\"input_ids\"]\n",
        "\n",
        "  return input\n",
        "\n",
        "train_df = train_df.apply(tokenize_text, axis=1)\n",
        "test_df = test_df.apply(tokenize_text, axis=1)"
      ],
      "metadata": {
        "id": "kabtlp-IOT5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
      ],
      "metadata": {
        "id": "anm0xY-GOiLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer,TrainingArguments\n",
        "\n",
        "trainingargs = TrainingArguments(\n",
        "    output_dir=\"./results\",          # output directory for checkpoints\n",
        "    num_train_epochs=20,              # number of training epochs\n",
        "    per_device_train_batch_size=8,   # batch size per device during training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir=\"./logs\",            # directory for storing logs\n",
        "    logging_steps=50,                # how often to log training info\n",
        "    save_steps=500,                  # how often to save a model checkpoint\n",
        "    eval_steps=50,                   # how often to run evaluation\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_df,\n",
        "    eval_dataset=test_df,\n",
        "    args=trainingargs,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "J2g-MgmQPFrx",
        "outputId": "852138f9-caf8-416a-a451-205b43f4bddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdulraheemabdullah859\u001b[0m (\u001b[33mabdulraheemabdullah859-aha-dynamics\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_073845-smpf45mz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface/runs/smpf45mz' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface' target=\"_blank\">https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface/runs/smpf45mz' target=\"_blank\">https://wandb.ai/abdulraheemabdullah859-aha-dynamics/huggingface/runs/smpf45mz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  3/360 00:59 < 5:56:47, 0.02 it/s, Epoch 0.11/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  6/360 03:07 < 4:36:28, 0.02 it/s, Epoch 0.28/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./chatbot_model\")\n",
        "tokenizer.save_pretrained(\"./chatbot_model\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./chatbot_model\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./chatbot_model\")"
      ],
      "metadata": {
        "id": "MOsHSQbQPyZJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "ea0cbf11-9353-4377-f301-acb806454bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-482d05d6eb30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./chatbot_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./chatbot_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./chatbot_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./chatbot_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = model.device\n",
        "def chatbot(query):\n",
        "  query = clean_text(query)\n",
        "  input_ids = tokenizer(query,return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=250).input_ids\n",
        "  outputs = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      num_beams=5,\n",
        "      early_stopping=True,\n",
        "      max_length=500,\n",
        "  )\n",
        "  return tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
        "\n",
        "while True:\n",
        "  user = input(\"Client : \")\n",
        "  print(\"Abdulraheem\",chatbot(user))\n",
        "  if user == \"exit\":\n",
        "    break"
      ],
      "metadata": {
        "id": "vFuq-86GQuKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXFgzrR8RUdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}